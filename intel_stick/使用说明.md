
## 转换模型
1.转到Intel工具包目录,例如：
`~/program/intel/computer_vision_sdk/deployment_tools/model_optimizer`
2.祥看参数说明文档，示例：
`python3 mo_mxnet.py --input_model /home/chen/models/mxnet/final-0000.params -
-model_name final --input_shape [1,3,224,224] --data_type FP16 --mean_values [123,116,103]`
**注意点**
1）cpu支持FP32模型，不支持FP16模型，intel神经计算棒支持FP16不支持FP32，所以模型使用--data_type FP16转FP16模型（训练的模型是FP32也可以转）；
2）输入数据，对于cpu设备，直接将输入设为FP32并做预处理即可，但计算棒的预处理只能放到网络中。一种方法在训练模型时模型本身有预处理层，第二种方法在转换模型是加上--mean_values和--scale_values等制定。
3）目前发现使用intel计算棒作为计算推理设备时，输入数据精度只能设置为U8，当设为FP16为预测错误，FP32为VPU不支持。所以预处理通过--mean_values实现。输入数据中只放入无符号整型的图片数据。
